{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55374f7-59aa-4f65-8855-0ee4a6170351",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8989ad0f-429f-4aca-9d96-b1a440cd63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhil\\AppData\\Local\\Temp\\ipykernel_27128\\3197170562.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK datasets (if not already done)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de129c9c-ca9e-4c22-aa7a-19ac20ea82c4",
   "metadata": {},
   "source": [
    "## Step 2: Create a Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd1b1f3-20e7-4113-81ab-7e4b75903976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for customer segmentation\n",
    "customer_data = pd.DataFrame({\n",
    "    'CustomerID': [1, 2, 3, 4, 5],\n",
    "    'PurchaseAmount': [200, 150, 300, 400, 100],\n",
    "    'PurchaseFrequency': [3, 5, 2, 4, 6]\n",
    "})\n",
    "\n",
    "# Create synthetic text data for sentiment analysis\n",
    "text_data = pd.DataFrame({\n",
    "    'Review': [\n",
    "        \"I love the product, it's amazing!\",\n",
    "        \"Terrible experience, will never buy again.\",\n",
    "        \"Decent quality but not worth the price.\",\n",
    "        \"Excellent service and great value.\",\n",
    "        \"Worst product I have ever used.\"\n",
    "    ],\n",
    "    'Sentiment': [1, 0, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeeabc5-575d-4162-89ed-c9fec405c718",
   "metadata": {},
   "source": [
    "## Step 3: K-Means Clustering for Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9795a724-b315-4ca1-8391-639093fd4c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.5765558837701512\n",
      "   CustomerID  PurchaseAmount  PurchaseFrequency  Cluster\n",
      "0           1             200                  3        0\n",
      "1           2             150                  5        0\n",
      "2           3             300                  2        1\n",
      "3           4             400                  4        1\n",
      "4           5             100                  6        0\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for clustering\n",
    "features = customer_data[['PurchaseAmount', 'PurchaseFrequency']]\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "customer_data['Cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(features, customer_data['Cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Display clusters\n",
    "print(customer_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96ef92-a054-4217-9d58-9e02cfb074a7",
   "metadata": {},
   "source": [
    "## Step 4: Text Preprocessing for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e21b96-5840-48f8-abe7-f50b46c2d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Review               Processed_Review\n",
      "0           I love the product, it's amazing!           love product amazing\n",
      "1  Terrible experience, will never buy again.  terrible experience never buy\n",
      "2     Decent quality but not worth the price.     decent quality worth price\n",
      "3          Excellent service and great value.  excellent service great value\n",
      "4             Worst product I have ever used.        worst product ever used\n"
     ]
    }
   ],
   "source": [
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to text data\n",
    "text_data['Processed_Review'] = text_data['Review'].apply(preprocess_text)\n",
    "print(text_data[['Review', 'Processed_Review']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10339b5-5015-4869-a14b-2bb0221e1de5",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c383d04-1913-404f-90e3-6d332e826155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(text_data['Processed_Review'])\n",
    "\n",
    "# Extract labels\n",
    "y = text_data['Sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6f3eb-3903-445f-bf9e-e8f5c19d2997",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea99d4e0-ef6c-4fa9-be84-5487e606be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78861d-ed2a-47d7-91eb-50bdda14e91a",
   "metadata": {},
   "source": [
    "## Step 7: Build and Evaluate Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023bd852-fb7d-4dbb-9c8f-261fbb0dc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe0f4e-ce78-451e-98e7-0a06d92409c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
